{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TES-RNN-GPU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"ES-NN","language":"python","name":"es-nn"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","source":["# ***Installation Requirements***\n","\n"],"metadata":{"id":"l8Q8pYyRqFQo"}},{"cell_type":"code","metadata":{"id":"3Y1czngSveec"},"source":["!pip install tensorflow==1.14"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***Mount Google Drive***"],"metadata":{"id":"h9E0aE5QrdZY"}},{"cell_type":"code","metadata":{"id":"qA4zUfv_vkBp"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My\\ Drive/wowmom22_tes-rnn/Anomaly_Detection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***Imports***"],"metadata":{"id":"2ypeVqGAuSRZ"}},{"cell_type":"code","metadata":{"id":"4dkRuMNhqwFw"},"source":["import math\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from data_loading import create_dataset, Dataset\n","from config import get_config\n","from trainer import TESRNNTrainer\n","from validator import TESRNNValidator\n","from tester import TESRNNTester\n","from model import TESRNN\n","from loss_modules import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***TES-RNN***"],"metadata":{"id":"mBI-C5mczUjW"}},{"cell_type":"code","metadata":{"id":"4TKcS_0NreF_"},"source":["# CONFIGURATION SETTINGS\n","\n","# List of the services to be tested\n","services = ['Facebook', 'Instagram', 'Snapchat', 'Twitter', 'YouTube']\n","\n","# Base Station to be tested\n","bs_file = '/(35,3).npy'\n","bs_folder = '/BS_(35,3)'\n","\n","# Number of clusters (single cell prediction)\n","num_clusters = 1\n","\n","# List of dropout probabilities to be tested\n","ps_dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","# Validation runs for uncertainty\n","uncertainty_runs = 10 \n","\n","# Test runs for uncertainty\n","uncertainty_test_runs = 100\n","\n","# Define the number of training epochs\n","epochs = 100\n","\n","# Define the number of training batch size\n","batch_size = 288\n","\n","# Define the number of train, validation and test samples\n","train_samples = 16128\n","val_samples = 4032\n","test_samples = 2016\n","\n","# Define the input size and output size of the prediction\n","input_size = 6\n","output_size = 1\n","\n","# Golden ratio for the golden search algorithm\n","gratio = (math.sqrt(5) + 1) / 2\n","\n","# Stopping condition value for the golden search algorithm (interval length)\n","stop_value = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SIMULATION RUNS\n","num_runs = 10\n","\n","# Simulations over different services\n","for service in services:\n","\n","    # Simulations over different dropout probabilities\n","    for p_dropout in ps_dropout:\n","\n","        # Configuration loading\n","        config = get_config('Traffic', epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, input_size, output_size)\n","    \n","        # Data loading\n","        data = '../../Dataset/' + service + bs_file\n","        train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'])\n","        dataset = Dataset(train, val, test, config['device'])\n","    \n","        # Maximum of single cluster traffic in the training set (for normalization)\n","        maximum = np.max(train[0])\n","    \n","\n","        # Running many simulations for a given service\n","        for i in range(1, num_runs+1):\n","\n","            # Initial extremes of the interval of the Minimum Level Threshold tau (expressed as fraction of maximum)\n","            tau_min = 0.0\n","            tau_max = 1.0\n","    \n","            # Current extremes of the interval of tau\n","            c = tau_min\n","            d = tau_max\n","    \n","            # Iterations counter for golden search algorithm\n","            iterations = 1\n","    \n","            # Dictionary collecting denormalized validation loss values for a given tau\n","            val_dict = {}\n","    \n","\n","            # Stopping condition for golden search algorithm\n","            while abs(tau_max - tau_min) > stop_value:\n","        \n","                # Determine current Minimum Level Threshold tau\n","                if (iterations%3) > 0:\n","                    # Try tau as left extreme    \n","                    if (iterations%3) == 1:\n","                        tau = c\n","                    # Try tau as right extreme\n","                    else:\n","                        tau = d\n","        \n","\n","                # Run actual golden search algorithm \n","                else:\n","\n","                    # Determine the new extreme of tau interval\n","                    if f_c < f_d:\n","                        # print(\"\\nNew right-extreme of the interval is %f\" % d)\n","                        tau_max = d\n","                    else:\n","                        # print(\"\\nNew left-extreme of the interval is %f\" % c)\n","                        tau_min = c\n","                \n","                    # print(\"Current length of tau interval is %f \\n\" % abs(tau_max - tau_min))\n","                    c = tau_max - (tau_max - tau_min) / gratio\n","                    d = tau_min + (tau_max - tau_min) / gratio\n","                    iterations = iterations + 1\n","                    continue\n","        \n","\n","                # Compute denormalized validation loss for current tau\n","                f_val = val_dict.get(round(tau,6))\n","                # print(\"\\nSearching a threshold in the interval [%f,%f]\" % (tau_min, tau_max))\n","                # print(\"Threshold for this run is %f\" % tau)\n","        \n","\n","                # Denormalized validation loss not yet calculated for current tau\n","                if f_val == None:\n","        \n","                    # Dataloader initialization\n","                    dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","\n","                    # Model initialization\n","                    run_id = service + bs_folder + '/Dropout_' + str(p_dropout) + '/Simulation_' + str(i)\n","                    model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id, p_dropout = p_dropout)\n","\n","                    # Run model trainer\n","                    trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","                    trainer.train_epochs()\n","    \n","                    # Run model validator with uncertainty\n","                    uncertain_val_losses = []\n","                    for val_run in range(uncertainty_runs):\n","                      validator = TESRNNValidator(model, dataloader, run_id, config)\n","                      validator.validating()\n","        \n","                      # Compute denormalized validation loss\n","                      norm_preds = np.load('Results/' + run_id + '/val_predictions.npy')\n","                      norm_actuals = np.load('Results/' + run_id + '/val_actuals.npy')\n","                      levels = np.load('Results/' + run_id + '/val_levels.npy')\n","                      val_loss = denorm_validation_loss(norm_preds, norm_actuals, levels)\n","                      uncertain_val_losses.append(val_loss)\n","\n","                    val_loss = np.average(np.array(uncertain_val_losses))\n","                    # print(\"Denormalized validation loss for this run %f\" % val_loss)\n","                    val_dict[round(tau,6)] = val_loss\n","\n","                    # Set denormalized validation loss for interval extreme\n","                    if (iterations%3) == 1:\n","                        f_c = val_loss\n","                    else:\n","                        f_d = val_loss\n","        \n","\n","                # Denormalized validation loss already calculated for current tau\n","                else:\n","                    # print(\"Denormalized validation loss for this run %f\" % f_val)\n","                    # Set denormalized validation loss for interval extreme\n","                    if (iterations%3) == 1:\n","                        f_c = f_val\n","                    else:\n","                        f_d = f_val\n","            \n","\n","                # Increase algorithm iterations\n","                iterations = iterations + 1\n","\n","        \n","    \n","            # Get the final optimal Minimum Level Threshold tau\n","            tau = (tau_min + tau_max) / 2\n","            # print('\\nFinally chosen threshold = %f\\n' % tau)\n","            np.save('Results/' + run_id + '/optimal_tau.npy', tau)\n","    \n","\n","\n","            # Run the optimized model\n","    \n","            # Dataloader initialization\n","            dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n","    \n","            # Model initialization\n","            model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id, p_dropout = p_dropout)\n","    \n","            # Run model trainer\n","            trainer = TESRNNTrainer(model, dataloader, run_id, config)\n","            trainer.train_epochs()\n","    \n","            # Run model tester with uncertainty\n","            uncertain_test_losses = []\n","            for test_run in range(uncertainty_test_runs):\n","              tester = TESRNNTester(model, dataloader, run_id, config, test_run)\n","              tester.testing()"],"metadata":{"id":"FkKJiRG10k0w"},"execution_count":null,"outputs":[]}]}