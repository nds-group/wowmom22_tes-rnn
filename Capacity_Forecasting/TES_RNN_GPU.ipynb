{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TES-RNN-GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l8Q8pYyRqFQo",
        "h9E0aE5QrdZY",
        "2ypeVqGAuSRZ"
      ]
    },
    "kernelspec": {
      "display_name": "ES-NN",
      "language": "python",
      "name": "es-nn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Installation Requirements***\n",
        "\n"
      ],
      "metadata": {
        "id": "l8Q8pYyRqFQo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y1czngSveec"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Mount Google Drive***"
      ],
      "metadata": {
        "id": "h9E0aE5QrdZY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA4zUfv_vkBp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/wowmom22_tes-rnn/Capacity_Forecasting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Imports***"
      ],
      "metadata": {
        "id": "2ypeVqGAuSRZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dkRuMNhqwFw"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from data_loading import create_dataset, Dataset\n",
        "from config import get_config\n",
        "from trainer import TESRNNTrainer\n",
        "from validator import TESRNNValidator\n",
        "from tester import TESRNNTester\n",
        "from model import TESRNN\n",
        "from loss_modules import *"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***TES-RNN***"
      ],
      "metadata": {
        "id": "mBI-C5mczUjW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TKcS_0NreF_"
      },
      "source": [
        "# CONFIGURATION SETTINGS\n",
        "\n",
        "# List of the services to be tested\n",
        "services = ['Facebook', 'Instagram', 'Snapchat', 'Twitter', 'YouTube']\n",
        "\n",
        "# Number of clusters\n",
        "num_clusters = 1\n",
        "\n",
        "# List of alphas to be tested\n",
        "alphas = [1, 2, 3, 5, 10]\n",
        "\n",
        "# Define the number of training epochs\n",
        "epochs = 100\n",
        "\n",
        "# Define the number of training batch size\n",
        "batch_size = 288\n",
        "\n",
        "# Define the number of train, validation and test samples\n",
        "train_samples = 16128\n",
        "val_samples = 4032\n",
        "test_samples = 2016\n",
        "\n",
        "# Define the input size and output size of the prediction\n",
        "input_size = 6\n",
        "output_size = 1\n",
        "\n",
        "# Golden ratio for the golden search algorithm\n",
        "gratio = (math.sqrt(5) + 1) / 2\n",
        "\n",
        "# Stopping condition value for the golden search algorithm (interval length)\n",
        "stop_value = 0.01"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMULATION RUNS\n",
        "num_runs = 10\n",
        "\n",
        "# Simulations over different services\n",
        "for service in services:\n",
        "\n",
        "    # Simulations over different alpha values\n",
        "    for alpha in alphas:\n",
        "\n",
        "        # Configuration loading\n",
        "        config = get_config('Traffic', epochs, num_clusters, batch_size, train_samples, val_samples, test_samples, alpha, input_size, output_size)\n",
        "    \n",
        "        # Data loading\n",
        "        data = '../../Dataset/' + service + '/time_load_cor_matrix.npy'\n",
        "        train, val, test = create_dataset(data, config['chop_train'], config['chop_val'], config['chop_test'])\n",
        "        dataset = Dataset(train, val, test, config['device'])\n",
        "    \n",
        "        # Maximum of single cluster traffic in the training set (for normalization)\n",
        "        maximum = np.max(train[0])\n",
        "    \n",
        "\n",
        "        # Running many simulations for a given service and alpha\n",
        "        for i in range(1, num_runs+1):\n",
        "\n",
        "            # Initial extremes of the interval of the Minimum Level Threshold tau (expressed as fraction of maximum)\n",
        "            tau_min = 0.0\n",
        "            tau_max = 1.0\n",
        "    \n",
        "            # Current extremes of the interval of tau\n",
        "            c = tau_min\n",
        "            d = tau_max\n",
        "    \n",
        "            # Iterations counter for golden search algorithm\n",
        "            iterations = 1\n",
        "    \n",
        "            # Dictionary collecting denormalized validation loss values for a given tau\n",
        "            val_dict = {}\n",
        "    \n",
        "\n",
        "            # Stopping condition for golden search algorithm\n",
        "            while abs(tau_max - tau_min) > stop_value:\n",
        "        \n",
        "                # Determine current Minimum Level Threshold tau\n",
        "                if (iterations%3) > 0:\n",
        "                    # Try tau as left extreme    \n",
        "                    if (iterations%3) == 1:\n",
        "                        tau = c\n",
        "                    # Try tau as right extreme\n",
        "                    else:\n",
        "                        tau = d\n",
        "        \n",
        "\n",
        "                # Run actual golden search algorithm \n",
        "                else:\n",
        "\n",
        "                    # Determine the new extreme of tau interval\n",
        "                    if f_c < f_d:\n",
        "                        # print(\"\\nNew right-extreme of the interval is %f\" % d)\n",
        "                        tau_max = d\n",
        "                    else:\n",
        "                        # print(\"\\nNew left-extreme of the interval is %f\" % c)\n",
        "                        tau_min = c\n",
        "                \n",
        "                    # print(\"Current length of tau interval is %f \\n\" % abs(tau_max - tau_min))\n",
        "                    c = tau_max - (tau_max - tau_min) / gratio\n",
        "                    d = tau_min + (tau_max - tau_min) / gratio\n",
        "                    iterations = iterations + 1\n",
        "                    continue\n",
        "        \n",
        "\n",
        "                # Compute denormalized validation loss for current tau\n",
        "                f_val = val_dict.get(round(tau,6))\n",
        "                # print(\"\\nSearching a threshold in the interval [%f,%f]\" % (tau_min, tau_max))\n",
        "                # print(\"Threshold for this run is %f\" % tau)\n",
        "        \n",
        "\n",
        "                # Denormalized validation loss not yet calculated for current tau\n",
        "                if f_val == None:\n",
        "        \n",
        "                    # Dataloader initialization\n",
        "                    dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n",
        "\n",
        "                    # Model initialization\n",
        "                    run_id = service + '/Alpha_' + str(alpha) + '/Simulation_' + str(i)\n",
        "                    model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id)\n",
        "\n",
        "                    # Run model trainer\n",
        "                    trainer = TESRNNTrainer(model, dataloader, run_id, config)\n",
        "                    trainer.train_epochs()\n",
        "    \n",
        "                    # Run model validator\n",
        "                    validator = TESRNNValidator(model, dataloader, run_id, config)\n",
        "                    validator.validating()\n",
        "        \n",
        "                    # Compute denormalized validation loss\n",
        "                    norm_preds = np.load('Results/' + run_id + '/val_predictions.npy')\n",
        "                    norm_actuals = np.load('Results/' + run_id + '/val_actuals.npy')\n",
        "                    levels = np.load('Results/' + run_id + '/val_levels.npy')\n",
        "                    val_loss = denorm_validation_loss(norm_preds, norm_actuals, levels, alpha)\n",
        "                    # print(\"Denormalized validation loss for this run %f\" % val_loss)\n",
        "                    val_dict[round(tau,6)] = val_loss\n",
        "\n",
        "                    # Set denormalized validation loss for interval extreme\n",
        "                    if (iterations%3) == 1:\n",
        "                        f_c = val_loss\n",
        "                    else:\n",
        "                        f_d = val_loss\n",
        "        \n",
        "\n",
        "                # Denormalized validation loss already calculated for current tau\n",
        "                else:\n",
        "                    # print(\"Denormalized validation loss for this run %f\" % f_val)\n",
        "                    # Set denormalized validation loss for interval extreme\n",
        "                    if (iterations%3) == 1:\n",
        "                        f_c = f_val\n",
        "                    else:\n",
        "                        f_d = f_val\n",
        "            \n",
        "\n",
        "                # Increase algorithm iterations\n",
        "                iterations = iterations + 1\n",
        "\n",
        "        \n",
        "    \n",
        "            # Get the final optimal Minimum Level Threshold tau\n",
        "            tau = (tau_min + tau_max) / 2\n",
        "            # print('\\nFinally chosen threshold = %f\\n' % tau)\n",
        "            np.save('Results/' + run_id + '/optimal_tau.npy', tau)\n",
        "    \n",
        "\n",
        "\n",
        "            # Run the optimized model\n",
        "    \n",
        "            # Dataloader initialization\n",
        "            dataloader = DataLoader(dataset, batch_size=config['series_batch'], shuffle=False)\n",
        "    \n",
        "            # Model initialization\n",
        "            model = TESRNN(tau = tau, maximum = maximum, num_clusters = num_clusters, config = config, run_id = run_id)\n",
        "    \n",
        "            # Run model trainer\n",
        "            trainer = TESRNNTrainer(model, dataloader, run_id, config)\n",
        "            trainer.train_epochs()\n",
        "    \n",
        "            # Run model tester\n",
        "            tester = TESRNNTester(model, dataloader, run_id, config)\n",
        "            tester.testing()"
      ],
      "metadata": {
        "id": "FkKJiRG10k0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}